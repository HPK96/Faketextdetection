
#Labels are binary 1 is real and 0 is Fake 

#practicing regilar expressions 
import re
import pandas as pd
import nltk
from nltk.tokenize import sent_tokenize as st, word_tokenize as wt
from sklearn.feature_extraction.text import CountVectorizer
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import string
from nltk.corpus import stopwords
import numpy as np
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

#---------------------Using cropped data-------------------------------
basiccsv=pd.read_csv('fake_or_real_news.csv')
data_split=np.random.rand(len(basiccsv))<0.50# take true/false table to split the data, all true will go traiting , all false will fo testing data
df_with_links=basiccsv[~data_split]
print(df_with_links)
#---------------------Using cropped data-------------------------------

#df_testing=pd.read_csv('capstone_file.csv')
print('first')
print(df_with_links['label'].value_counts())

#PREPROCESSING 
nltk.download('stopwords')
stop_word
      return any(char.isdigit() for char in inputString)
    no_dig_tweets=[]  
    temp_no_dig_tweets=[] 
    for tw_1 in txt:
      for this_word in tw_1.split():
        if not hasNumbers(this_word):
          temp_no_dig_tweets.append(this_word.lower())
        word=" ".join(temp_no_dig_tweets)
      no_dig_tweets.append(word)
      temp_no_dig_tweets=[]

    for tweet in no_dig_tweets:
        text_remove_punt=[char for char in tweet if char not in string.punctuation] #in char is not in punctuation list then return
        text_gather_chars_into_words=''.join(text_remove_punt).split()#make 1 long string and split into words
        text_remove_stop_words=[w for w in text_gather_chars_into_words if w.lower() not in stop_words] #if word not in list then  return 
        cleaned_tweets.append(text_remove_stop_words)
    return cleaned_tweets

# method to remove duplicate tweets 
def remove_duplicates(df_for_removing_duplicates):
    no_dub=pd.DataFrame.drop_duplicates(df_for_removing_duplicates)
    return no_dub

#METHOD CALLS
no_duplicates=remove_duplicates(df_with_links)
print('no dublicates first one ')
print(no_duplicates['label'].value_counts())

rem=[]   #list
remove_urls_at_hash(new_list=rem,data_column=no_duplicates['text'])
clean_data=text_cleaning(rem) # removing punctuation, stop words

#to make a from a nested list regular list
clean_data_list=[]
for ii in clean_data:
    clean_data_list.append(' '.join(ii))
print('len of the clean data after all pre processing is :')
print(len(clean_data_list))

final_clean_df=pd.DataFrame(clean_data_list) # casting into Data Frame, so we can use it for vectorization
final_clean_set=list(final_clean_df[0])


#Beg of Words using CountVectorizer from sklearn


vectorizer=CountVectorizer()
x=vectorizer.fit_transform(final_clean_set)#get vocab then build vector then make an array
features_extracted=vectorizer.vocabulary_
#print(features_extracted) # gives the vocablulary list
print('training data shape is')
print(x.shape) # representation of (tweet number, voablulary list)

# i need list [] --> pu ldfghdfabels in the list 
future_label=[]fghg
# DEVIDE INTO TRAINING AND TESTING SET 
data_split=np.random.radfg(len(array_to_df))<0.80# take true/false table to split the data, all true will go traiting , all false will fo testing data
train=array_to_df[data_split] # training set based on randomly selested values 80%
test=array_to_df[~data_split] # testing set based on 20% randomly seleted values
print(' Size of training set : ', len(train))gfh
print(' Size dfgchbdfghof testigng]
y_train=train.iloc[:,leng]dfgdfsg
dfgdfdfg
# split into independednt and dependednt value fgdfgr testingdf
from sklearn.neighbors import KNeighborsClassifier
from sklearn import metrics
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
Ks = 10
for n in range(1,Ks):tching metric is ",metrics.accuracy_score(y_test, yhat)) dfg'])
  plt.figure(figsize = (10, 10))
  sns.heatmap(cm, annot = True,fmt='d')dfggdfg
  print(" using ma

from sklearn.linegdf_model import LogisticRegression
LR = LogisticRegression(C=0.5, solvefgd',metrics.accuracy_score(y_test,yhat ))
df
d sklearn.naive_bayes import ComplementNB
model_ComplementNB = ComplementNB(norm="True").fit(x_train, y_train)
yhat = model_gf.predict(x_test)
print('ComplementNB accuracy_score : ',metrics.accuracy_score(y_test, yhat))f
plt.figure(figsize = (10, 10))
sns.heatmap(cm, annot = True,fmt=dfgdf')

#sklearn modelsfg

from sklearn.linear_model import PassiveAggressiveClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
from mlxtend.plotting import plot_confusion_matrix
from collections import Counter
from nltk.tokenize import word_tokenize
from sklearn.metrics import classification_report
gdfgressiveClassifier(max_iter=300)
model1.fit(x_train,y_train)
y_pred1=model1.predict(x_test)df

model3=Decisifassifier',acc3)df
sns.heatmap(cm, annot = True,fmt='d')

model4=RandomForestClassifier()
model4.fit(x_train,y_train)
y_pred4=model4.predict(x_test)g
cm = pd.crosstab(y_pred4,y_test,rownames=['Predicted'], colnames=['Actual'])
plt.figure(figsize = (10, 10))
sns.heatmap(cm, annot = True,fmt='d')

model5=SVC()
model5.fit(x_train,y_traindf

cm = pd.crosstab(y_pred5,y_test,rownames=['Predicted'], colnames=['Actual'])
plt.figure(figsize = (10, 10))
sns.heatmap(cm, annot = True,fmt='d')

model6=LogisticRegression()
model6.fit(x_train,y_train)
y_pred6=model6.predict(x_test)
acc6=accuracy_score(y_test,y_pred6)
print('LogisticRegression',acc6)

cm = pd.crosstab(y_pred6,y_test,rownames=['Predicted'], colnames=['Actual'])
plt.figure(figsize = (10, 10))
sns.heatmap(cm, annot = True,fmt='d')
print(classification_report(y_test,y_pred6))
